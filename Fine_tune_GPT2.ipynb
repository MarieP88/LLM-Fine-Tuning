{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b204867e-42a2-460e-9703-04c85d9896ab",
   "metadata": {},
   "source": [
    "## Fine tuning Chat GPT 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c295dbd1-efd3-4f86-82b2-a4e535c24ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f62436d-af0c-433b-860b-1e6dbc1305b1",
   "metadata": {},
   "source": [
    "### Install Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569f9466-e311-41f2-b107-60f6d72baec0",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e69778d-ee09-43bb-a144-2f6b80d5a834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af63dff-218b-4bee-a67b-aafda4528641",
   "metadata": {},
   "source": [
    "### Load the Pre-trained Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d1064e-065a-44e2-9875-4566fbc5589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and tokenizer\n",
    "model_name = 'gpt2'  # You can use 'gpt2-medium', 'gpt2-large', or 'gpt2-xl' for larger models\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa853973-96d8-4379-8b57-58ddec6cb6ba",
   "metadata": {},
   "source": [
    "### Prepare Your Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39012851-9934-43ed-b33d-82db56824f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a text dataset from a local file (replace 'path/to/your/dataset.txt' with your dataset path)\n",
    "# The text file should contain your training examples separated by new lines\n",
    "\n",
    "dataset = load_dataset('text', data_files='path/to/your/dataset.txt')\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871a9070-ad66-4103-b4e4-c50121dce63b",
   "metadata": {},
   "source": [
    "### Set Up Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be8159e8-716b-46b6-8cb6-45d41f5cff64",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TrainingArguments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      2\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m'\u001b[39m,          \u001b[38;5;66;03m# output directory for the model predictions and checkpoints\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,     \u001b[38;5;66;03m# evaluate every epoch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-5\u001b[39m,              \u001b[38;5;66;03m# learning rate\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,   \u001b[38;5;66;03m# batch size for training\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,              \u001b[38;5;66;03m# number of training epochs\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,               \u001b[38;5;66;03m# strength of weight decay\u001b[39;00m\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TrainingArguments' is not defined"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory for the model predictions and checkpoints\n",
    "    evaluation_strategy=\"epoch\",     # evaluate every epoch\n",
    "    learning_rate=2e-5,              # learning rate\n",
    "    per_device_train_batch_size=2,   # batch size for training\n",
    "    num_train_epochs=3,              # number of training epochs\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaeebaa-921a-40c3-806e-b682773cf9e6",
   "metadata": {},
   "source": [
    "### Create the Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13330025-544c-4ce3-a769-15b818b6cce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=tokenized_dataset['train'],  # training dataset\n",
    "    eval_dataset=tokenized_dataset['train']  # evaluation dataset (using the same for simplicity)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40e2a4e-f0f3-4713-8c9b-0ce096a0f1ae",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce0407f-0883-40a0-95f5-d26bededbf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d41755-906d-4d17-82f2-b5f48a3b62b4",
   "metadata": {},
   "source": [
    "### Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723ce433-cc80-4c0e-8feb-3f4bf920218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained('./fine_tuned_gpt2')\n",
    "tokenizer.save_pretrained('./fine_tuned_gpt2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b52b2e-0785-4390-880c-da5aa4950fe8",
   "metadata": {},
   "source": [
    "###  Using the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a507b67-0924-4e7d-af34-72857af025db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "fine_tuned_model = GPT2LMHeadModel.from_pretrained('./fine_tuned_gpt2')\n",
    "fine_tuned_tokenizer = GPT2Tokenizer.from_pretrained('./fine_tuned_gpt2')\n",
    "\n",
    "# Generate text using the fine-tuned model\n",
    "input_text = \"Natural Language Processing\"\n",
    "input_ids = fine_tuned_tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Move to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    input_ids = input_ids.to('cuda')\n",
    "\n",
    "output = fine_tuned_model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "generated_text = fine_tuned_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f02f38-ccd3-4461-85c0-6581ce5c9f38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
